{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4zJdUg41B84",
        "outputId": "4991422a-44dd-43b4-e704-1f0e5efb3215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas matplotlib scikit-learn tensorflow torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlcQ3Adr1UnN",
        "outputId": "b32588c2-5c84-46f3-afa6-d57b0e222c33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0jnIBwS1qJP",
        "outputId": "bb409b0e-0ba3-4672-cda3-7863bb5a33d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FPjGMfTC258o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4541f31b-97fc-4766-e8dd-5fda2a059c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 02:07:19--  https://thor.robots.ox.ac.uk/~vgg/data/text/\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 308 Permanent Redirect\n",
            "Location: https://thor.robots.ox.ac.uk/text/ [following]\n",
            "--2025-05-23 02:07:20--  https://thor.robots.ox.ac.uk/text/\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2025-05-23 02:07:20 ERROR 403: Forbidden.\n",
            "\n",
            "Archive:  synth90k.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of synth90k.zip or\n",
            "        synth90k.zip.zip, and cannot find synth90k.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "#download and prepare the dataset\n",
        "!wget -O synth90k.zip https://thor.robots.ox.ac.uk/~vgg/data/text/\n",
        "!unzip synth90k.zip -d ./synth90k/\n"
      ]
    },
    {
      "source": [
        "# --- Start of Code Cell 4: download_dataset ---\n",
        "# Assistant: Generate a synthetic dataset of 200 images directly in the notebook.\n",
        "# This avoids downloading the very large MJSynth dataset and ensures a runnable example.\n",
        "\n",
        "print(\"Generating a synthetic dataset of 200 images...\")\n",
        "\n",
        "dataset_root = './synthetic_ocr_dataset'\n",
        "os.makedirs(dataset_root, exist_ok=True)\n",
        "image_dir = os.path.join(dataset_root, 'images')\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Define image dimensions\n",
        "IMAGE_HEIGHT = 32\n",
        "IMAGE_WIDTH = 128\n",
        "\n",
        "# Function to generate a random word\n",
        "def generate_random_word(min_len=3, max_len=10):\n",
        "    length = random.randint(min_len, max_len)\n",
        "    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length))\n",
        "\n",
        "# Function to create a synthetic image of a word\n",
        "def create_synthetic_image(word, filename, size=(IMAGE_WIDTH, IMAGE_HEIGHT)):\n",
        "    img = Image.new('L', size, color=255) # White background (L for grayscale)\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    # Try to use a common font available in Colab, or fallback\n",
        "    try:\n",
        "        # Path to a common font on Colab VMs (might vary slightly)\n",
        "        font_path = \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\"\n",
        "        font = ImageFont.truetype(font_path, 20)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default() # Fallback to default PIL font\n",
        "\n",
        "    # Calculate text size to center it\n",
        "    bbox = d.textbbox((0,0), word, font=font)\n",
        "    text_width = bbox[2] - bbox[0]\n",
        "    text_height = bbox[3] - bbox[1]\n",
        "\n",
        "    x = (size[0] - text_width) / 2\n",
        "    y = (size[1] - text_height) / 2\n",
        "\n",
        "    d.text((x, y), word, fill=0, font=font) # Black text\n",
        "    img.save(os.path.join(image_dir, filename))\n",
        "\n",
        "# Generate 200 images\n",
        "num_images_to_generate = 200\n",
        "all_image_paths = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(num_images_to_generate):\n",
        "    word = generate_random_word().lower() # Generate lowercase words\n",
        "    filename = f\"{word}_{i}.png\" # Format like MJSynth for consistency\n",
        "    create_synthetic_image(word, filename)\n",
        "    all_image_paths.append(os.path.join(image_dir, filename))\n",
        "    all_labels.append(word)\n",
        "\n",
        "print(f\"Generated {len(all_image_paths)} synthetic images in {image_dir}\")\n",
        "\n",
        "# Shuffle the generated images and labels for good measure\n",
        "combined = list(zip(all_image_paths, all_labels))\n",
        "random.shuffle(combined)\n",
        "all_image_paths, all_labels = zip(*combined)\n",
        "all_image_paths = list(all_image_paths)\n",
        "all_labels = list(all_labels)\n",
        "\n",
        "# Define character set dynamically from the generated labels\n",
        "all_chars_in_labels = sorted(list(set(''.join(all_labels))))\n",
        "# It's good practice to ensure common characters are included, especially if random generation is limited\n",
        "# For a robust OCR, you'd typically include 'abcdefghijklmnopqrstuvwxyz0123456789' and possibly punctuation.\n",
        "# For this synthetic data, we'll rely on what's generated.\n",
        "characters = all_chars_in_labels\n",
        "\n",
        "char_to_num = tf.keras.layers.StringLookup(vocabulary=list(characters), mask_token=None)\n",
        "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
        "\n",
        "# Add 1 for the blank token used in CTC\n",
        "num_classes = char_to_num.vocabulary_size() + 1 # +1 for CTC blank token\n",
        "print(f\"Number of unique characters (classes including blank): {num_classes}\")\n",
        "print(f\"Character vocabulary (excluding blank): {char_to_num.get_vocabulary()}\")\n",
        "\n",
        "# --- End of Code Cell 4: download_dataset ---"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9iu8B0i6ILx",
        "outputId": "8b5de881-d1cc-41da-f154-f6173eae1b51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating a synthetic dataset of 200 images...\n",
            "Generated 200 synthetic images in ./synthetic_ocr_dataset/images\n",
            "Number of unique characters (classes including blank): 38\n",
            "Character vocabulary (excluding blank): ['[UNK]', np.str_('0'), np.str_('1'), np.str_('2'), np.str_('3'), np.str_('4'), np.str_('5'), np.str_('6'), np.str_('7'), np.str_('8'), np.str_('9'), np.str_('a'), np.str_('b'), np.str_('c'), np.str_('d'), np.str_('e'), np.str_('f'), np.str_('g'), np.str_('h'), np.str_('i'), np.str_('j'), np.str_('k'), np.str_('l'), np.str_('m'), np.str_('n'), np.str_('o'), np.str_('p'), np.str_('q'), np.str_('r'), np.str_('s'), np.str_('t'), np.str_('u'), np.str_('v'), np.str_('w'), np.str_('x'), np.str_('y'), np.str_('z')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 5: preprocessing_functions ---\n",
        "# Preprocessing Function: Resize images, normalize pixel values, and prepare labels\n",
        "\n",
        "IMAGE_HEIGHT = 32\n",
        "IMAGE_WIDTH = 128\n",
        "\n",
        "def encode_single_sample(img_path, label):\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1) # Or tf.io.decode_jpeg if your synthetic images are JPG\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
        "    # 5. Transpose the image because the CRNN expects (width, height, channels)\n",
        "    # This is a common practice for CRNNs with CTC loss, as it aligns timesteps with width.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2]) # (width, height, channels)\n",
        "\n",
        "    # 6. Encode label to numbers\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding='UTF-8'))\n",
        "\n",
        "    return img, label\n",
        "\n",
        "print(\"Preprocessing functions defined.\")\n",
        "\n",
        "# --- End of Code Cell 5: preprocessing_functions ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbJYEXxn6vYK",
        "outputId": "bd5a78d3-a1b2-46fb-c7bb-c1b9ae9b96f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uH0cP7wD3Muy"
      },
      "outputs": [],
      "source": [
        "#Preprocessing Function: Resize images, normalize pixel values, and prepare labels\n",
        "def preprocess_image(image_path, target_size=(128, 32)):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    image = cv2.resize(image, target_size)\n",
        "    image = np.expand_dims(image, axis=-1) / 255.0  # Normalize\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 6: extract_labels ---\n",
        "# Extract Labels for OCR Training:\n",
        "def extract_labels(image_path):\n",
        "    # Assuming filenames follow 'img_label_id.jpg' or similar, as in the dummy data.\n",
        "    # For MJSynth, labels are typically part of the directory structure or a separate annotation file.\n",
        "    try:\n",
        "        label = os.path.basename(image_path).split(\"_\")[0].lower() # Corrected to [0] for synthetic data\n",
        "    except IndexError:\n",
        "        label = \"UNKNOWN\" # Fallback for incorrect format\n",
        "    return label\n",
        "\n",
        "# --- End of Code Cell 6: extract_labels ---"
      ],
      "metadata": {
        "id": "MLNm5lKD7K2f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 7: data_augmentation ---\n",
        "# OCR models improve accuracy with augmentation to simulate real-world distortions.\n",
        "# Data augmentation helps prevent overfitting and improves model robustness.\n",
        "# Note: Horizontal flip is generally not suitable for text recognition.\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomRotation(factor=0.02), # Small rotation\n",
        "    tf.keras.layers.RandomBrightness(factor=0.1), # Adjust brightness\n",
        "    tf.keras.layers.RandomContrast(factor=0.1), # Adjust contrast\n",
        "    # Consider adding RandomZoom or RandomTranslation if appropriate for your data\n",
        "])\n",
        "\n",
        "print(\"Data augmentation pipeline defined.\")\n",
        "\n",
        "# --- End of Code Cell 7: data_augmentation ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW7zDD1c7TCZ",
        "outputId": "f9f25c05-59b6-47fd-cc16-be8e76188444"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation pipeline defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 8: build_crnn_model ---\n",
        "# CRNN combines CNN for feature extraction and RNN for sequential text decoding.\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_crnn_model(input_shape, num_classes):\n",
        "    # Defines the input layer of the model.\n",
        "    # input_shape is expected to be (width, height, channels) for CRNN.\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- CNN Feature Extractor (Convolutional Neural Network) ---\n",
        "    # This part extracts visual features from the input image.\n",
        "    # It's a common pattern in image processing, often resembling a VGG-like architecture.\n",
        "\n",
        "    # First Convolutional Block\n",
        "    # Conv2D: Applies a 2D convolution. 32 filters, 3x3 kernel, ReLU activation, 'same' padding to maintain size.\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    # MaxPooling2D: Reduces spatial dimensions (width and height) by taking the maximum value over a 2x2 window.\n",
        "    # This helps in downsampling and making the model more robust to small shifts.\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # --- Reshape for RNN (Recurrent Neural Network) ---\n",
        "    # The output of the CNN is a 4D tensor (batch_size, new_width, new_height, num_filters).\n",
        "    # RNNs (LSTMs) expect a 3D tensor (batch_size, timesteps, features_per_timestep).\n",
        "    # For CRNN, 'timesteps' typically corresponds to the width dimension of the image,\n",
        "    # and 'features_per_timestep' combines the height and the number of filters.\n",
        "\n",
        "    # Calculate the dimensions after the CNN layers and pooling operations.\n",
        "    # Each MaxPooling2D with pool_size=(2,2) halves the width and height.\n",
        "    cnn_output_width = input_shape[0] // (2 * 2 * 2) # Input width / 2^3\n",
        "    cnn_output_height = input_shape[1] // (2 * 2 * 2) # Input height / 2^3\n",
        "    num_filters_last_conv = 128 # The number of filters in the last Conv2D layer\n",
        "\n",
        "    # Check to ensure the height hasn't been reduced to zero, which would cause issues.\n",
        "    if cnn_output_height == 0:\n",
        "        raise ValueError(\"Image height is too small for the current CNN architecture. \"\n",
        "                         \"Consider reducing pooling layers or increasing input_height.\")\n",
        "\n",
        "    # Reshape the tensor: (batch_size, cnn_output_width, cnn_output_height * num_filters_last_conv)\n",
        "    x = Reshape((cnn_output_width, cnn_output_height * num_filters_last_conv))(x)\n",
        "\n",
        "    # --- RNN Layers for Sequence Modeling ---\n",
        "    # These layers process the sequence of features extracted by the CNN to predict the text.\n",
        "    # Bidirectional LSTM: Processes the sequence in both forward and backward directions,\n",
        "    # capturing context from both past and future elements in the sequence.\n",
        "    # return_sequences=True: Ensures the LSTM outputs a sequence, not just a single vector,\n",
        "    # which is necessary because the next layer (another LSTM or Dense) needs a sequence.\n",
        "    # dropout: Applies dropout to the inputs and recurrent connections to prevent overfitting.\n",
        "\n",
        "    # First Bidirectional LSTM Layer\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.25))(x) # Increased LSTM units for more capacity\n",
        "\n",
        "    # Second Bidirectional LSTM Layer\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    # Dense layer: Maps the LSTM outputs to character probabilities for each timestep.\n",
        "    # num_classes: The total number of unique characters in your vocabulary plus one for the CTC blank token.\n",
        "    # activation=\"softmax\": Outputs a probability distribution over all possible characters for each timestep.\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the Keras Model.\n",
        "    # This model takes the 'inputs' (image) and produces 'outputs' (character probabilities over timesteps).\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "print(\"CRNN model architecture defined.\")\n",
        "\n",
        "# --- End of Code Cell 8: build_crnn_model ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ssa1QZ7X3Q",
        "outputId": "19040003-a925-4b4d-be38-d1037bfc6855"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN model architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8TVAXHZCZF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# --- Start of Code Cell 9: ctc_loss_implementation ---\n",
        "# CTC Loss Implementation\n",
        "\n",
        "class CTCLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Keras layer to compute the CTC loss.\n",
        "    CTC loss is suitable for sequence-to-sequence problems where the alignment\n",
        "    between input and output sequences is not known (e.g., OCR).\n",
        "    \"\"\"\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        # Use the backend CTC loss function\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "\n",
        "    # This method helps Keras infer the output shape of the layer.\n",
        "    # Since `call` returns `y_pred`, its output shape is the same as `y_pred`'s input shape.\n",
        "    # input_shape here is a list/tuple of shapes for all inputs to the call method:\n",
        "    # (y_true_shape, y_pred_shape, input_length_shape, label_length_shape)\n",
        "    # We want the shape of y_pred, which is input_shape[1].\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Return the shape of the second input to the call method (y_pred's shape)\n",
        "        # This is the most robust way to indicate that the layer passes through its second input's shape.\n",
        "        # The output shape should be the shape of the predictions (y_pred) as this layer\n",
        "        # effectively adds a loss and passes the predictions through.\n",
        "        return input_shape[1]\n",
        "\n",
        "\n",
        "    def call(self, y_true, y_pred, input_length, label_length):\n",
        "        \"\"\"\n",
        "        Computes the CTC loss and adds it to the model's total loss.\n",
        "\n",
        "        Args:\n",
        "            y_true (tf.Tensor): True labels (ground truth sequences).\n",
        "                                Shape: (batch_size, max_label_length)\n",
        "            y_pred (tf.Tensor): Predicted logits from the model (output of the RNN).\n",
        "                                Shape: (batch_size, timesteps, num_classes)\n",
        "            input_length (tf.Tensor): Lengths of the input sequences (width of image features).\n",
        "                                      Shape: (batch_size, 1)\n",
        "            label_length (tf.Tensor): Lengths of the true label sequences.\n",
        "                                      Shape: (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Ensure input_length and label_length are 1D tensors of type int32\n",
        "        input_length = tf.squeeze(input_length, axis=-1)\n",
        "        label_length = tf.squeeze(label_length, axis=-1)\n",
        "\n",
        "        # Cast y_true to int32, as ctc_batch_cost expects integer labels.\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "        # Compute the CTC loss for the current batch\n",
        "        # The loss function expects (y_true, y_pred, input_length, label_length)\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "        # Add the computed loss to the model's total loss.\n",
        "        # This is the standard way to add a loss calculated inside a custom layer.\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Return y_pred. This layer primarily adds a loss, but Keras requires layers\n",
        "        # to have an output. Returning the prediction tensor allows the model to\n",
        "        # connect subsequent layers (if any) or serve as the final output for a training model\n",
        "        # where the loss is handled internally.\n",
        "        return y_pred\n",
        "\n",
        "print(\"CTC Layer defined.\")\n",
        "\n",
        "# Define the CRNN model builder function\n",
        "def build_crnn_model_with_ctc(input_shape, num_classes):\n",
        "    # Input for images (batch_size, width, height, channels)\n",
        "    inputs = Input(shape=input_shape, name='image_input')\n",
        "\n",
        "    # --- CNN feature extractor ---\n",
        "    # Extracts visual features. VGG-like architecture.\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x) # Output shape (None, W/2, H/2, 32)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x) # Output shape (None, W/4, H/4, 64)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x) # Output shape (None, W/8, H/8, 128)\n",
        "\n",
        "    # Calculate dimensions after pooling\n",
        "    cnn_output_width = input_shape[0] // (2 * 2 * 2) # Input width / 8\n",
        "    cnn_output_height = input_shape[1] // (2 * 2 * 2) # Input height / 8\n",
        "    num_filters_last_conv = 128\n",
        "\n",
        "    # Add check for zero height after pooling\n",
        "    if cnn_output_height == 0:\n",
        "         raise ValueError(f\"Image height ({input_shape[1]}) is too small for the current CNN architecture. \"\n",
        "                          f\"After 3x MaxPooling(2,2), height becomes {cnn_output_height}. \"\n",
        "                          \"Consider reducing pooling layers or increasing input_height.\")\n",
        "\n",
        "    # Reshape for RNN: (batch_size, timesteps, features_per_timestep)\n",
        "    # Timesteps = cnn_output_width\n",
        "    # Features_per_timestep = cnn_output_height * num_filters_last_conv\n",
        "    x = Reshape((cnn_output_width, cnn_output_height * num_filters_last_conv))(x) # Output shape (None, W/8, H/8 * 128)\n",
        "\n",
        "    # --- RNN Layers for Sequence Modeling ---\n",
        "    # Bidirectional LSTMs process sequence in both directions.\n",
        "    # return_sequences=True: Pass sequence output to the next layer.\n",
        "    # dropout: Prevent overfitting.\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.25))(x) # Output shape (None, W/8, 2*256)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x) # Output shape (None, W/8, 2*128)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    # Dense layer maps LSTM outputs to character probabilities (logits) for each timestep.\n",
        "    # num_classes: Total unique characters + blank token.\n",
        "    # activation=\"softmax\": Provides probability distribution over classes.\n",
        "    output_logits = Dense(num_classes, activation=\"softmax\", name='output_logits')(x) # Output shape (None, W/8, num_classes)\n",
        "\n",
        "    # --- Inputs for CTC Loss ---\n",
        "    # These inputs are required by the CTCLayer during training to compute the loss.\n",
        "    # labels: Ground truth sequences (numerical IDs). Shape (None, max_label_length).\n",
        "    # input_length: Lengths of the prediction sequences (width after CNN). Shape (None, 1).\n",
        "    # label_length: Lengths of the ground truth sequences. Shape (None, 1).\n",
        "    labels = Input(name='labels', shape=(None,), dtype='float32') # Use float32, will cast to int32 in CTCLayer\n",
        "    input_length = Input(name='input_length', shape=(1,), dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=(1,), dtype='int64')\n",
        "\n",
        "    # --- CTC Layer ---\n",
        "    # Instantiate the custom CTCLayer.\n",
        "    ctc_layer = CTCLayer(name='ctc_loss')\n",
        "\n",
        "    # Call the CTCLayer with the necessary inputs.\n",
        "    # This call computes the CTC loss and adds it to the model's total loss via `add_loss`.\n",
        "    # The output of the CTCLayer call (`loss_out`) serves as the output of the *training* model,\n",
        "    # allowing Keras to build the graph correctly and incorporate the added loss.\n",
        "    loss_out = ctc_layer(labels, output_logits, input_length, label_length)\n",
        "\n",
        "    # --- Define Models ---\n",
        "    # Model for training: Takes image, labels, input_length, label_length as inputs.\n",
        "    # Its output is the result of the CTCLayer call (which effectively passes the logits through\n",
        "    # but ensures the layer is part of the graph and its loss is considered).\n",
        "    train_model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out, name='crnn_train_model')\n",
        "\n",
        "    # Model for inference: Takes only the image input.\n",
        "    # Its output is the raw character probabilities (logits) from the Dense layer,\n",
        "    # which will be decoded externally using CTC decoding algorithms.\n",
        "    inference_model = Model(inputs=inputs, outputs=output_logits, name='crnn_inference_model')\n",
        "\n",
        "    return train_model, inference_model\n",
        "\n",
        "print(\"CRNN model builder with CTC defined.\")\n",
        "\n",
        "# --- End of Code Cell 9: ctc_loss_implementation ---"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8EQSHh0Csk9",
        "outputId": "c037fdcc-6cfb-4ee8-fd91-b368e26527fa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CTC Layer defined.\n",
            "CRNN model builder with CTC defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 9: ctc_loss_implementation ---\n",
        "# CTC Loss Implementation\n",
        "\n",
        "class CTCLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Keras layer to compute the CTC loss.\n",
        "    CTC loss is suitable for sequence-to-sequence problems where the alignment\n",
        "    between input and output sequences is not known (e.g., OCR).\n",
        "    \"\"\"\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "\n",
        "    # This method helps Keras infer the output shape of the layer.\n",
        "    # Since `call` returns `y_pred`, its output shape is the same as `y_pred`'s input shape.\n",
        "    # input_shape here is a list/tuple of shapes for all inputs to the call method:\n",
        "    # (y_true_shape, y_pred_shape, input_length_shape, label_length_shape)\n",
        "    # We want the shape of y_pred, which is input_shape[1].\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Return the shape of the second input to the call method (y_pred's shape)\n",
        "        # This is the most robust way to indicate that the layer passes through its second input's shape.\n",
        "        return input_shape[1]\n",
        "\n",
        "    def call(self, y_true, y_pred, input_length, label_length):\n",
        "        \"\"\"\n",
        "        Computes the CTC loss and adds it to the model's total loss.\n",
        "\n",
        "        Args:\n",
        "            y_true (tf.Tensor): True labels (ground truth sequences).\n",
        "                                Shape: (batch_size, max_label_length)\n",
        "            y_pred (tf.Tensor): Predicted logits from the model (output of the RNN).\n",
        "                                Shape: (batch_size, timesteps, num_classes)\n",
        "            input_length (tf.Tensor): Lengths of the input sequences (width of image features).\n",
        "                                      Shape: (batch_size, 1)\n",
        "            label_length (tf.Tensor): Lengths of the true label sequences.\n",
        "                                      Shape: (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Ensure input_length and label_length are 1D tensors of type int32\n",
        "        input_length = tf.squeeze(input_length, axis=-1)\n",
        "        label_length = tf.squeeze(label_length, axis=-1)\n",
        "\n",
        "        # Cast y_true to int32, as ctc_batch_cost expects integer labels.\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "        # Compute the CTC loss for the current batch\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "        # Add the computed loss to the model's total loss.\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Return y_pred as a placeholder.\n",
        "        return y_pred\n",
        "\n",
        "print(\"CTC Layer and model builder with CTC defined.\")\n",
        "\n",
        "# Re-define the model to include the CTCLayer for training\n",
        "def build_crnn_model_with_ctc(input_shape, num_classes):\n",
        "    # Input for images\n",
        "    inputs = Input(shape=input_shape, name='image_input')\n",
        "\n",
        "    # CNN feature extractor\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    cnn_output_width = input_shape[0] // (2 * 2 * 2)\n",
        "    cnn_output_height = input_shape[1] // (2 * 2 * 2)\n",
        "    num_filters_last_conv = 128\n",
        "\n",
        "    x = Reshape((cnn_output_width, cnn_output_height * num_filters_last_conv))(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.25))(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer for character probabilities\n",
        "    output_logits = Dense(num_classes, activation=\"softmax\", name='output_logits')(x)\n",
        "\n",
        "    # Inputs for CTC Loss calculation during training\n",
        "    labels = Input(name='labels', shape=(None,), dtype='float32')\n",
        "    input_length = Input(name='input_length', shape=(1,), dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=(1,), dtype='int64')\n",
        "\n",
        "    # CTC Layer\n",
        "    loss_out = CTCLayer(name='ctc_loss')(labels, output_logits, input_length, label_length)\n",
        "\n",
        "    # Model for training\n",
        "    train_model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    # Model for inference (without CTC layer)\n",
        "    inference_model = Model(inputs=inputs, outputs=output_logits)\n",
        "\n",
        "    return train_model, inference_model\n",
        "\n",
        "print(\"CTC Layer and model builder with CTC defined.\")\n",
        "\n",
        "# --- End of Code Cell 9: ctc_loss_implementation ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG7RKSTNBc9a",
        "outputId": "24f5664d-a831-407b-f846-76df1608d2c3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CTC Layer and model builder with CTC defined.\n",
            "CTC Layer and model builder with CTC defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 10: create_datasets ---\n",
        "# Create tf.data.Dataset objects\n",
        "\n",
        "# Split data (for real dataset, use proper train/val/test split)\n",
        "# 'all_image_paths' and 'all_labels' are populated by Code Cell 4 (your synthetic data generation).\n",
        "num_samples = len(all_image_paths)\n",
        "train_split = int(0.8 * num_samples) # 80% for training\n",
        "val_split = int(0.9 * num_samples)   # 10% for validation (from 80% to 90%)\n",
        "\n",
        "# Divide the image paths and labels into training, validation, and test sets.\n",
        "train_img_paths = all_image_paths[:train_split]\n",
        "train_labels = all_labels[:train_split]\n",
        "\n",
        "val_img_paths = all_image_paths[train_split:val_split]\n",
        "val_labels = all_labels[train_split:val_split]\n",
        "\n",
        "test_img_paths = all_image_paths[val_split:] # Remaining 10% for testing\n",
        "test_labels = all_labels[val_split:]\n",
        "\n",
        "# Define the batch size for training.\n",
        "# BATCH_SIZE = 4 is used for the small dummy data; for real training, this would typically be larger (e.g., 32 or 64).\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "def prepare_dataset(img_paths, labels):\n",
        "    \"\"\"\n",
        "    Prepares a tf.data.Dataset from image paths and labels.\n",
        "\n",
        "    Args:\n",
        "        img_paths (list): List of file paths to the images.\n",
        "        labels (list): List of corresponding text labels.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A TensorFlow dataset ready for model consumption.\n",
        "    \"\"\"\n",
        "    # 1. Create a dataset from slices of image paths and labels.\n",
        "    # Each element in the dataset will initially be (img_path, label_string).\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
        "\n",
        "    # 2. Map the preprocessing function to each element.\n",
        "    # 'encode_single_sample' (from Code Cell 5) reads the image, decodes it,\n",
        "    # resizes, transposes, and encodes the label into numerical format.\n",
        "    # num_parallel_calls=tf.data.AUTOTUNE allows TensorFlow to optimize parallel processing.\n",
        "    dataset = dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # 3. Batch the dataset.\n",
        "    # Combines consecutive elements into batches of BATCH_SIZE.\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    # 4. Add input_length and label_length for CTC Loss.\n",
        "    # The CTCLayer (from Code Cell 9) requires these lengths.\n",
        "    # x: image tensor (batch_size, width, height, channels)\n",
        "    # y: label tensor (batch_size, max_label_length)\n",
        "    # tf.shape(x)[1]: gets the width of the image features, which is the sequence length for CTC.\n",
        "    # tf.shape(y)[1]: gets the length of the encoded label sequence.\n",
        "    dataset = dataset.map(lambda x, y: (x, y, tf.shape(x)[1], tf.shape(y)[1]))\n",
        "\n",
        "    # 5. Prefetch data.\n",
        "    # Prefetching overlaps data preprocessing and model execution,\n",
        "    # ensuring that the next batch of data is ready when the model finishes the current step.\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create the training, validation, and test datasets using the prepare_dataset function.\n",
        "train_dataset = prepare_dataset(train_img_paths, train_labels)\n",
        "val_dataset = prepare_dataset(val_img_paths, val_labels)\n",
        "test_dataset = prepare_dataset(test_img_paths, test_labels)\n",
        "\n",
        "print(\"tf.data.Dataset objects created.\")\n",
        "\n",
        "# --- End of Code Cell 10: create_datasets ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brsHB6Ok7xhh",
        "outputId": "bd4dd095-e655-4679-deae-33931265e9dc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.data.Dataset objects created.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# --- Start of Code Cell 11: train_model ---\n",
        "# Use CTC (Connectionist Temporal Classification) Loss, Adam optimizer, and Early Stopping\n",
        "\n",
        "# Define the input shape for the CRNN model.\n",
        "# IMAGE_WIDTH and IMAGE_HEIGHT are defined in Code Cell 5.\n",
        "input_shape = (128, 21, 1) # CRNN expects (width, height, channels)\n",
        "\n",
        "# Build the training and inference models using the function from Code Cell 9.\n",
        "# This function returns two models: one for training (with CTCLayer) and one for inference (without CTCLayer).\n",
        "train_model, inference_model = build_crnn_model_with_ctc(input_shape, num_classes)\n",
        "\n",
        "# Print a summary of the training model's architecture.\n",
        "# This is useful for reviewing the layers, output shapes, and parameter counts.\n",
        "train_model.summary()\n",
        "\n",
        "# Define the learning rate schedule.\n",
        "# CosineDecay: A learning rate schedule that decays the learning rate following a cosine curve.\n",
        "# This often helps models converge better than a fixed learning rate.\n",
        "# initial_learning_rate: The starting learning rate.\n",
        "# decay_steps: The number of steps over which the learning rate will decay.\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.0001, decay_steps=1000)\n",
        "\n",
        "# Define the optimizer.\n",
        "# Adam: A popular optimization algorithm known for its efficiency.\n",
        "# learning_rate: Uses the defined cosine decay schedule.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile the training model.\n",
        "# Note: The loss is handled internally by the CTCLayer (from Code Cell 9).\n",
        "# Therefore, we do NOT specify a 'loss' argument in model.compile.\n",
        "# The `train_model`'s output (which is `loss_out` from CTCLayer) implicitly provides the loss.\n",
        "# Metrics like 'accuracy' for CTC are complex and are typically handled in a custom evaluation step\n",
        "# (as seen in Code Cell 12) rather than directly as a Keras metric during compilation.\n",
        "train_model.compile(optimizer=optimizer)\n",
        "\n",
        "# Define Early Stopping callback.\n",
        "# EarlyStopping: A callback that monitors a specified metric (here, 'val_loss')\n",
        "# and stops training if the metric stops improving for a certain number of epochs ('patience').\n",
        "# monitor=\"val_loss\": Monitors the validation loss.\n",
        "# patience=5: Training will stop if validation loss does not improve for 5 consecutive epochs.\n",
        "# restore_best_weights=True: After stopping, the model's weights will be reset to the epoch\n",
        "# that had the best monitored value (lowest 'val_loss').\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "# Train the model.\n",
        "# x=train_dataset: The dataset used for training.\n",
        "# validation_data=val_dataset: The dataset used to evaluate validation loss and metrics.\n",
        "# epochs=20: The maximum number of training iterations over the entire dataset.\n",
        "# callbacks=[early_stopping]: The list of callbacks to apply during training.\n",
        "# Ensure that Code Cell 10 defining train_dataset and val_dataset has been executed.\n",
        "history = train_model.fit(\n",
        "    x=train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20, # Adjust epochs based on dataset size and convergence\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- End of Code Cell 11: train_model ---"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "3vWZWnLg8BsE",
        "outputId": "9268f71a-0c03-4dda-c0de-67c125bbdbc9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling CTCLayer.call().\n\n\u001b[1mMethod `compute_output_shape()` of layer CTCLayer is returning a type that cannot be interpreted as a shape. It should return a shape tuple. Received: None\u001b[0m\n\nArguments received by CTCLayer.call():\n  • args=('<KerasTensor shape=(None, None), dtype=float32, sparse=False, name=labels>', '<KerasTensor shape=(None, 16, 38), dtype=float32, sparse=False, name=keras_tensor_152>', '<KerasTensor shape=(None, 1), dtype=int64, sparse=False, name=input_length>', '<KerasTensor shape=(None, 1), dtype=int64, sparse=False, name=label_length>')\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-e0e2b527d4fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Build the training and inference models using the function from Code Cell 9.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This function returns two models: one for training (with CTCLayer) and one for inference (without CTCLayer).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_crnn_model_with_ctc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Print a summary of the training model's architecture.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-631602e4e8f1>\u001b[0m in \u001b[0;36mbuild_crnn_model_with_ctc\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# The output of the CTCLayer call (`loss_out`) serves as the output of the *training* model,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# allowing Keras to build the graph correctly and incorporate the added loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mloss_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# --- Define Models ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mcompute_output_spec\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1085\u001b[0m                         \u001b[0;34m\"Method `compute_output_shape()` of layer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m                         \u001b[0;34mf\"{self.__class__.__name__} is returning \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling CTCLayer.call().\n\n\u001b[1mMethod `compute_output_shape()` of layer CTCLayer is returning a type that cannot be interpreted as a shape. It should return a shape tuple. Received: None\u001b[0m\n\nArguments received by CTCLayer.call():\n  • args=('<KerasTensor shape=(None, None), dtype=float32, sparse=False, name=labels>', '<KerasTensor shape=(None, 16, 38), dtype=float32, sparse=False, name=keras_tensor_152>', '<KerasTensor shape=(None, 1), dtype=int64, sparse=False, name=input_length>', '<KerasTensor shape=(None, 1), dtype=int64, sparse=False, name=label_length>')\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of Code Cell 12: evaluate_and_plot ---\n",
        "# Compare results with other OCR solutions & Plot accuracy trends\n",
        "\n",
        "# Function to decode CTC output (for evaluation)\n",
        "def decode_batch_predictions(pred, num_to_char_layer):\n",
        "    \"\"\"\n",
        "    Decodes the raw predictions (logits) from the CRNN model into readable text.\n",
        "    Uses CTC greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        pred (tf.Tensor): The raw output probabilities from the inference model.\n",
        "                          Shape: (batch_size, timesteps, num_classes)\n",
        "        num_to_char_layer (tf.keras.layers.StringLookup): The layer to convert\n",
        "                                                          numerical IDs back to characters.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of decoded text strings for the batch.\n",
        "    \"\"\"\n",
        "    # Create an array of input lengths, which is the number of timesteps (width)\n",
        "    # for each prediction in the batch.\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "\n",
        "    # Use greedy search. For a production system, beam search might be better.\n",
        "    # tf.keras.backend.ctc_decode performs the decoding.\n",
        "    # greedy=True: Selects the most probable character at each timestep.\n",
        "    # The output is a tuple; we take the first element (decoded sequences)\n",
        "    # and then the first item from that (the actual sequences).\n",
        "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "\n",
        "    # Iterate over the decoded numerical results and convert them back to text.\n",
        "    output_texts = []\n",
        "    for res in results.numpy():\n",
        "        # Remove padding tokens (typically -1 from ctc_decode)\n",
        "        res = res[res != -1]\n",
        "        # Join the numerical character IDs into a string using num_to_char_layer\n",
        "        # and decode from bytes to utf-8.\n",
        "        output_texts.append(tf.strings.reduce_join(num_to_char_layer(res)).numpy().decode('utf-8'))\n",
        "    return output_texts\n",
        "\n",
        "# Evaluate on test data (requires a custom evaluation loop for CTC accuracy)\n",
        "print(\"Evaluating model on test dataset...\")\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "# Iterate through each batch in the test_dataset.\n",
        "for batch in test_dataset:\n",
        "    # Unpack the batch: images, true labels (numerical), input lengths, label lengths.\n",
        "    images, labels, input_length, label_length = batch\n",
        "\n",
        "    # Get predictions from the inference model (without CTC loss layer).\n",
        "    preds = inference_model.predict(images)\n",
        "    # Decode these raw predictions into human-readable text.\n",
        "    decoded_preds = decode_batch_predictions(preds, num_to_char)\n",
        "\n",
        "    # Decode the true labels (numerical) into human-readable text for comparison.\n",
        "    true_labels = []\n",
        "    for label_seq in labels.numpy():\n",
        "        label_seq = label_seq[label_seq != -1] # Remove padding\n",
        "        true_labels.append(tf.strings.reduce_join(num_to_char(label_seq)).numpy().decode('utf-8'))\n",
        "\n",
        "    # Compare decoded predictions with true labels.\n",
        "    for i in range(len(decoded_preds)):\n",
        "        total_samples += 1\n",
        "        # Check if the predicted text matches the true text (case-insensitive).\n",
        "        if decoded_preds[i].lower() == true_labels[i].lower():\n",
        "            correct_predictions += 1\n",
        "        # Print the true and predicted labels for visual inspection.\n",
        "        print(f\"True: {true_labels[i]}, Predicted: {decoded_preds[i]}\")\n",
        "\n",
        "# Calculate the overall test accuracy.\n",
        "test_accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}\") # Print accuracy formatted to two decimal places\n",
        "\n",
        "# Plot accuracy trends from history (loss is typically monitored for CTC)\n",
        "# Check if the 'history' object from model.fit is available and contains loss data.\n",
        "if history is not None and 'loss' in history.history:\n",
        "    plt.figure(figsize=(12, 6)) # Create a figure for the plots\n",
        "\n",
        "    # Plot Training and Validation Loss\n",
        "    plt.subplot(1, 2, 1) # 1 row, 2 columns, first plot\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title('CRNN OCR Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Note: 'accuracy' metric for CTC is complex and often requires custom implementation.\n",
        "    # For this basic setup, we're relying on the manual test accuracy calculation above.\n",
        "    # The commented-out section below shows how you *would* plot accuracy if it were\n",
        "    # a directly logged metric in history.\n",
        "    # plt.subplot(1, 2, 2) # 1 row, 2 columns, second plot\n",
        "    # plt."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "eoyahIUT9iQW",
        "outputId": "6e89164a-7505-4ea6-d0dc-020a285a1ceb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model on test dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9b7510e3ff9d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Iterate through each batch in the test_dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Unpack the batch: images, true labels (numerical), input lengths, label lengths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HgbI0AF3eDt"
      },
      "outputs": [],
      "source": [
        "#Extract Labels for OCR Training:\n",
        "def extract_labels(image_path):\n",
        "    label = os.path.basename(image_path).split(\"_\")[1]  # Assuming filenames follow 'img_label_id.jpg'\n",
        "    return label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5waviTg3kio"
      },
      "outputs": [],
      "source": [
        "#OCR models improve accuracy with augmentation to simulate real-world distortions.\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "    tf.keras.layers.RandomBrightness(0.03),\n",
        "    tf.keras.layers.RandomContrast(0.03),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42AmkP3G3rU9"
      },
      "outputs": [],
      "source": [
        "#CRNN combines CNN for feature extraction and RNN for sequential text decoding.\n",
        "def build_crnn_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # CNN feature extractor\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # RNN layers for sequence modeling\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cRqw12L30sm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "0406a214-720c-4742-8698-67de8ea8e6e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Input' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2b2041a9a2a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#use CTC (Connectionist Temporal Classification) Loss, Adam optimizer, and Early Stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m37\u001b[0m  \u001b[0;31m# 26 letters + 10 digits + space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_crnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6bf7f950a411>\u001b[0m in \u001b[0;36mbuild_crnn_model\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CRNN combines CNN for feature extraction and RNN for sequential text decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_crnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# CNN feature extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
          ]
        }
      ],
      "source": [
        "#use CTC (Connectionist Temporal Classification) Loss, Adam optimizer, and Early Stopping\n",
        "num_classes = 37  # 26 letters + 10 digits + space\n",
        "model = build_crnn_model((128, 32, 1), num_classes)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(loss=tfa.losses.CTCLoss(), optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[early_stopping])\n"
      ]
    },
    {
      "source": [
        "#CRNN combines CNN for feature extraction and RNN for sequential text decoding.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_crnn_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # CNN feature extractor\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # RNN layers for sequence modeling\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-DXp5xsg2HnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZiGWBsr3-aO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "332da9d3-0657-44b5-e9f3-42265e75b8ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8f2a4902adea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Compare results with other OCR solutions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_accuracy:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot accuracy trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "#Compare results with other OCR solutions.\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Plot accuracy trends\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"CRNN OCR Model Accuracy\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}